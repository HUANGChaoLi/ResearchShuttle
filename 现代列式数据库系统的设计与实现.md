[TOC]

## 摘要

在本文中，我们调查了有关列式数据库系统或列存储的最新研究，其中表的每个属性存储在单独的文件或存储区域中。分析型查询通常对表中的几列进行扫描和聚合，近年来随着对分析型查询的日益关注，此类数据库得到了复兴。列存储的主要优点是它只需访问查询所需的列即可响应此类查询。 我们特别关注了三个有影响力的研究原型，即 MonetDB[[46](#46)]，VectorWise[[18](#18)] 和 C-Store[[88](#88)]。这些系统已成为多个著名的商业列存储实施的基础。我们描述了它们的相似处和不同之处，并讨论了它们在压缩、延期物化、连接处理、矢量化和自适应索引（数据库分裂）上的特殊架构特征。

## 1. 介绍





## 4. 列存储内部与高级技术

​		在了解了前面几章所介绍的列存储基础知识之后，在本章中我们将更详细地讨论特定的功能特性，这些功能不仅可以一次只存储一列数据(column-at-a-time)，而且还可以将列存储与传统的数据库架构区分开来。特别是我们专注于矢量化处理(vectorized processing)、延迟物化(late materialization)、压缩和数据库分裂(database cracking)。

### 4.1 矢量化处理

​		数据库教科书通常对照查询执行层的两种策略，即“火山型”迭代器模型和完全物化模型，其中火山模型也称为单元组流式模型(tuple-at-a-time pipelining)。在单元组流式模型中，每次有单个元组被推入(push)到查询计划树中。 查询树中每个关系运算符的 **next()** 方法每次会生成一个新的元组，并通过在树中的子运算符上调用 **next()** 方法来获取输入数据。 该方法不仅在软件工程上设计得十分优雅，而且还兼备着在中间结果的物化量上的最小优势。

​		另一方面，在完全物化(full materialization)的模型中，每个查询运算符都是独立工作的，该模型在运算符的输入和输出中极大地耗费了（磁盘或RAM的）存储空间。 MonetDB 是为数不多的使用完全物化的数据库系统之一，它采用了 [BAT 代数](https://www.monetdb.org/Documentation/Manuals/MonetDB/KernelModules/Algebra) 的运算命令，旨在使运算符以及运算符之间的交互变得更加简单，进而提高 CPU 使用效率。 但是 MonetDB 也因此产生了大量中间结果，这导致查询中占据了过多的资源。

​		为了说明上述两个模型之间的差异，假设我们执行以下查询：*select avg(A) from R where A < 100*。当使用单元组流式模型时，选择运算符将开始将符合条件的元组一次一个地推入到聚合操作符中。 而在完全物化的情况下，选择运算符将首先完全扫描列 A，接着创建一个包含了所有符合条件的元组的中间结果，然后将其作为输入传递到聚合运算符中。 选择和聚合运算符都可以非常有效地用于 for 循环中，但是另一方面也需要物化较大的中间结果，这在非选择查询或者在大数据量的情况下，会导致内存空间溢出的新问题。

​		我们现在将注意力转向到 VectorWise 率先提出的第三种替代方法中，即“矢量化执行”(vectorized execution)，该方法在完全物化和单元组流式模型之间取得了平衡。该模型将查询的进度控制逻辑与数据处理进行了逻辑分离。 在控制流方面，矢量化处理中的运算符与单元组流式模型中的运算符相似，唯一的区别是每个运算符的 **next()** 方法都返回 N 个元组的矢量，而不是单个元组。 而在数据处理方面，其所谓的原始函数（用来执行实际工作的运算符，例如相加或比较数据值）看起来很像 MonetDB 的 BAT 代数，这些函数一次处理一份矢量数据(vector-at-a-time)。 因此，矢量化执行将流式处理模型（避免了较大的中间结果物化）与使 MonetDB 更快的数组循环代码模型结合在了一起。

​		矢量化处理中所使用的矢量典型大小是使其可以适用于 L1 高速缓存（在 VectorWise 的典型情况中 N = 1000），因为这样可以在内存的层次结构中最大程度地减少读写操作。 鉴于现代列存储通常一次只能处理单列的一个矢量（另请参见第 [4.4](#4.4 延迟物化) 节中的关于延迟物化的讨论），这意味着 L1 中仅需要一个矢量、可能的输出矢量和辅助数据结构。例如，具有多个谓词和多个列的查询通常会将谓词单独地应用到每一列中，因此一次只需要将单列的一个矢量放入缓存中（详细的带有延迟物化的查询处理示例在第 [4.4](#4.4 延迟物化) 节中介绍）。

​		矢量化有许多优点。 我们总结以下几个主要方面：

- **降低解释处理开销.** 与单元组流式模型相比，查询解释器在执行时的函数调用量减少了与矢量大小相等的数量。 在计算密集型的查询中（例如 TPC-H Q1），可以将性能提高两个数量级。
- **更好的缓存局部性.** VectorWise 通过调整矢量的大小，以便评估查询中的所需矢量均适用于 CPU 缓存。 如果选择的矢量大小过大（如在 MonetDB 中的矢量大小是表的大小），那么矢量将无法被缓存容纳，并且额外的内存流量会减慢查询速度。而在指令缓存方面，矢量化模型还可以在单元组处理和完全物化之间取得平衡；控制处理程序在相同原始函数中的停留时间和矢量大小相同，因此产生了指令的局部性。
- **编译器的优化机遇.** 如 MonetDB 的描述中所述，矢量化原语通常在数组上执行紧密的循环，这适用于某些生产级的编译器优化，而且这通常还会触发编译器生成 SIMD 指令。
- **块处理算法.**  数据处理算法可以处理 N 个元组的方案通常可以引发对逻辑算法的优化。例如，当检查某些条件时（例如，输出缓冲区是否已满），单元组执行模型需要对每个元组执行缓冲区检查，而矢量化算法可以首先检查输出缓冲区是否有空间容纳 N 个以上的输出结果，如果满足这种约束，那么接下来无需进行任何检查即可对矢量进行后续所有工作。
- **并行内存访问.** 对于矢量的不同值，在现代 CPU 上执行矢量化的紧密循环内存访问的算法可能会产生多个未处理的高速缓存未命中。这是因为当发生高速缓存未命中时，现代 CPU 可以在这种紧密循环中进行推测。而这在单元组架构中是不可能的，因为 CPU 在处理不同元组之间的延迟绑定的 API 调用会禁止这种情况。为了在现代计算机上获得良好的内存带宽，必须产生多个并发的高速缓存未命中。在文献 [[96](#96)] 中指出，我们有可能在所有主流关系数据库的运算符中使用矢量化的内存查找，例如排序、哈希表查找和哈希表探查。这种查找通常会导致高速缓存未命中，而在这种情况下，通过乱序推测来产生多个并行缓存未命中的代码执行速度通常要比非矢量化的内存查找快四倍。
- **性能分析**.  由于关系运算符的矢量化实现采用矢量化的方式执行所有表达式评估工作，即一次可以对成百上千的元组进行数组化处理(array-at-a-time)，因此保持每个矢量化操作的性能分析的测量开销成本很低（因为账面上的维护成本平摊到了到数百或数千个元组中）。这使得矢量化引擎可以提供有关 CPU 周期开销的高度详细的性能信息。
- **自适应执行.** 在性能分析上的优点基础之上，对矢量化原语的性能分析信息也可以在查询执行期间的运行时使用。例如，对于根据某些谓词条件所选择的所有值的子集矢量，Vectorwise 会在对矢量进行算术运算时进行自适应决策，判断是否只对被选定的元组进行迭代计算还是对数组中所有元组进行计算。后一种策略在执行额外的工作时会采用紧密循环的处理方式，而无需使用 if-then-else，而且可以在其中使用 SIMD 指令，因此在被选定元组的百分比相对较高时它的整体速度会更快。 VectorWise 的微观自适应性机制 [[77](#77)] 提出了使用运行时统计信息优化查询处理的概念。自适应“Multi Armed Bandid”算法的任务是在运行时为矢量化函数选择最佳的实现方案。在查询期间（这期间矢量化原语可能会被调用上百万次），它会定期测试所有可替代实现方案，并在随后的大多数时间中使用性能最佳的实现方案。这种方法可以通过多次链接同一函数、多形式编译和倾向化处理的方法，来抵御编译器和编译配置的差异以及硬件的更改，这种方法避免了成本模型的建模、维护和校准的需要，并且还可以在查询过程中针对数据分布的变化作出相应调整。

​		矢量化执行主要涉及查询运算符以及对流经查询执行树的实时元组数据的处理。存储管理器用于持久性存储的数据布局和查询执行器所使用的数据布局之间可能会有差别。在开发和分析 VectorWise 列存储上下文中对当前布局的矢量化执行时，也可以将该原理应用于行存储中，因为它不受存储管理器的束缚。实际上，行存储的以往研究早在1994年就开始尝试这种概念[[85](#85)]，其中的第一项工作似乎是为了尽量减少高速缓存未命中和指令未命中。而随后的工作为基于块的查询处理提出了更通用的解决方案[[74](#74)]，其甚至在传统单元组行存储的查询计划中添加了所谓的缓冲运算符[[95](#95)]，这种方案规定直到缓冲区已满时才能允许元组在执行计划树中传递，以通过这种方式来模拟阻塞的效果。

​		在文献[[101](#101)]中指出，矢量化查询执行系统可以在单个执行框架中轻松支持垂直（列）和水平（记录）的元组表示，甚至可以以混合模式来存储实时元组（某些列以记录的形式来聚集，而其他列仍保持垂直的形式）。这项研究还指出，存储格式会显着影响运算符的性能，其中操作符特征、硬件参数和数据分布将决定哪个存储格式是最有效的。通常，顺序操作符（例如投影、选择）在垂直矢量上工作最有效（利用自动的内存预取和 SIMD 指令优化），而随机的存取操作符（例如哈希连接或聚合）根据水平划分的记录块上效果最好（利用高速缓存的局部性）。由于使用矢量化执行来对水平和垂直格式之间进行转换很便宜，因此这有可能使查询计划将元组布局更改为查询计划的一部分（这意味着可能进行多次转换）。这为查询优化器的查询布局规划的开辟了新方向，其应当使用基于成本的估算模型来为查询执行计划的每个阶段确定最佳布局。

###4.2 压缩

​		直观地讲，存储在列中的数据比存储在行中的数据更具可压缩性。压缩算法在信息熵较低的数据（即数据值的局部性较高）上表现更好，而且来自同一列的值倾向于比来自不同列的值拥有更好的值局部性。

​		**一次压缩一列.** 例如，假设数据库表中包含有关于客户的信息（姓名、电话号码、电子邮件地址，邮寄地址等）。当所有数据以行的形式存储在一起时，意味着每个数据页都包含有关姓名、电话号码和地址等信息，以及我们必须将所有这些信息压缩在一起。而在另一方面，将数据存储在列中可以将所有姓名存储在一起、将所有电话号码存储在一起，等等。当然电话号码要比其他字段（例如电子邮件地址或姓名）更为相似。这带来了两方面的积极影响，而这些影响加强了列存储中对压缩的使用；首先，当仅存储单个属性的数据时，压缩算法可能使用相同的通用模式来压缩更多的数据，因为更多的相同类型的数据会存储在单个页面中，其次，更为相似的的数据意味着通常用于压缩的数据结构和代码等会更小，因此可以带来更好的压缩效果。此外，如果数据是按某列进行排序的（这在列存储的投影中很常见），那么该列将具有超压缩性（例如，相同值的游程可以进行游程编码）。

​		**利用额外的CPU周期.** 数据库系统通常的底层目标是性能，即要求尽可能快地处理一个或多个查询，而不是提高数据的压缩率。磁盘空间很便宜而且正在迅速地变得更加便宜，但是压缩确实可以提高性能（除了减少磁盘空间之外）。如果将数据进行压缩，那么在查询处理期间花费在 I/O 操作上的时间会变得更少，因为只需要将更少的数据从磁盘读入到内存中（也包括从内存读入到 CPU 中）。采用压缩的另一个重要动机是，CPU 的速度与内存带宽相比要快得多，因此访问数据的成本在 CPU 周期方面要比过去要高得多。更直观地讲，这意味着现在我们拥有更多的 CPU 周期可以用于快速解压缩数据，这比通过内存的层次结构来慢速（相比 CPU 周期而言）传输更大的未压缩数据要更好。

​		**定宽数组与 SIMD.** 鉴于我们正在尝试优化系统性能，这意味着针对压缩率进行优化的某些“较重的”压缩方案（例如 Lempel-Ziv、Human 编码或算术编码）不如“更轻量级的”方案，因为轻量级的方案可以通过牺牲压缩率来提高解压缩性能。通常我们首选将列值压缩为固定宽度更小的值的轻量级压缩方案（例外情况要谨慎处理），因为这样可以将压缩后的列当作数组。通过对这样的数组进行迭代（例如用于解压缩）可以利用现代 CPU 上的 SIMD 指令集来实现矢量化的并行性（如上所述），从而显着提高性能。在使用 SIMD 指令时，只要将它们压缩到固定宽度的密集数组中（以更好地适用于现代处理器的SIMD寄存器中），我们就可以使用一条指令来解压缩或处理多个压缩值，从而最大程度地提高并行度。由于列存储无论如何都会利用固定宽度的密集数组，因此即便使用未压缩的数据，我们也可以利用 SIMD 并行执行。 但是与不压缩列相比，在进行压缩后会有更多的压缩数据值将同时适用于 SIMD 寄存器中，因此我们可以使用一条指令处理更多的数据。例如，现代处理器具有通常一次适用于 4 个 4 字节整数的 SIMD 寄存器，因此，在这种情况下未经压缩的列存储一次可以处理 4 个值。而如果将数据压缩 2 倍，那么我们可以在 SIMD 寄存器中放入 8 个压缩整数，并且一次可以处理 8 个值，进而提高了并行度。

​		总的来说，压缩已被证明可以极大地提高现代列存储的性能，现在它已成为工业界中所有列存储的组成部分。 此外，有趣的是，由于压缩而在磁盘上多获得的额外存储空间可用于物化辅助的数据结构或副本，即在 C-Store 中提出的投影。反过来，由于现在的查询可以享受更好的访问方式，因此可以进一步提高性能。关于列存储压缩的相关研究已经有很多[[2](#2), [100](#100), [43](#43), [15](#15)]，其中大部分工作是在 C-Store 和 VectorWise 中进行的，同时在工业界中也取得了重大进展。 特别是，IBM BLINK 项目[[11](#11)]提出了所谓的频率划分方案，该方案提供了更紧密的在压缩和列存储架构上的集成方案。

​		**频率划分.** 频率划分的主要动机是在提高压缩率的同时，仍提供一种依赖固定宽度数组并可以利用矢量化的架构，这就要求我们对系统架构进行更紧密的压缩设计。通过对频率进行划分，我们可以对列值进行重组，例如在单列的每一页中我们都保持尽可能低的信息熵。为此，IBM BLINK 根据列中出现的值的频率重新组织每列，即频繁相同的值共同存储在同一页中。这使得系统可以对每一页使用紧凑的字典来进行字典压缩，同时也仅需要更少的编码，而这些编码又可以使用更少的位来进行存储（相比与整一列只有一个字典的情况）。例如，如果我们只需要在一列的单个页面中区分两个值，那么此页面中的字典编码只需要一位。而在每个页面中，所有值/编码都是固定宽度的，这允许运算符具有 CPU 和高速缓存友好型的访问模式，这就像典型的列存储架构一样，系统以一次一页的粒度来执行矢量化处理 。

​		**压缩算法.** 有一些研究评估了在列存储中使用不同压缩算法的相关性能[[2](#2), [100](#100), [43](#43), [42](#42), [15](#15)]。这些算法中的某些是完全通用的，它们可以同时在行存储和列存储中使用。 但是有些是专门为列存储而设计的，因为它们允许压缩符号跨越同一列内的多个连续值（这在行存储中会出现问题，因为在行存储中，来自同一列的连续值不会在存储空间中连续存储）。

​		我们可以采用多种可能的压缩方案，即游程编码(run-length encoding)，位向量编码(bit-vector encoding)，字典压缩(dictionary compression)和修补(patching)。 我们将在后续各节中介绍这些技术。

#### 4.2.1 Run-length Encoding (游程编码)

​		游程编码(RLE)将列中相同值的游程压缩为紧凑的单一表示。 因此它非常适合已排序的列或者相同值的游程十分合理的列。这些游程使用三元组来表示: (值, 起始位置, 游程)，其中三元组的每个元素通常都是固定的位数。例如，如果某列的前 42 个元素都包含值“M”，那么可以将这些 42 个元素替换为三元组: (“M”, 1, 42)。

​		当在面向行的系统中使用时，RLE 仅用于具有大量空格或重复字符的大型字符串属性中。但是 RLE 可以更广泛地应用于面向列的系统，因为这些系统中的属性值是连续存储的，而且相同值通常具有十分合理的游程（尤其是在属性只有很少不同值的列的情况下）。例如，在 C-Store 的投影中，每列可能以多种投影/排序来进行存储，很多列最终都会被排序（或第二次排序），因此有很多机会使用 RLE 类型的编码。

​		假设 RLE 使用三元组来替换任意值的块，它可能会导致可变宽度和可变长度的列。这意味着我们不能使用先前描述的用于固定宽度列的运算符，并且元组重建会变得更加复杂。这是我们必须进行的存储收益权衡，因此 RLE 基于数据分布的情况在特定列上实现 I/O 和性能的改进。

#### 4.2.2 Bit-Vector Encoding (位向量编码)

​		当列具有有限数量的可能数据值时（例如，美国的州或 flag 列），位向量编码是最有用的。但同时，如果我们进一步压缩位向量，它甚至可以用于具有大量值的列中。在这种类型的编码中，位向量（其位数等于列的大小）在列的值域中与每个可能的唯一元素相关联，如果位向量的第 i 个位置的值等于该位向量相关联的域值，那么置为“1”，否则置为“0”。 例如，对于以下的列数据而言：

​		`1 1 3 2 2 3 1`

​		将会表示为三个位向量：

- 值为 1 的位向量：`1100001`
- 值为 2 的位向量：`0001100`
- 值为 3 的位向量：`0010010`

​		由于该方案的扩展版本可用于为行存储建立索引（即所谓的位图索引[[71](#71)]），因此在进一步压缩这部分位图以及压缩对查询性能的影响等方面已有大量工作 [[68](#68), [8](#8), [7](#7), [53](#53), [91](#91), [93](#93), [92](#92)]。

#### 4.2.3 Dictionary (字典)

​		字典编码在一些少量频繁值的分布中非常有效，也可以应用于字符串中。最简单的形式是根据以频率进行值排序的列构造一个字典表，并将该值表示为字典表的整数位置。这些整数可以再次使用整数压缩方案来进行压缩。全局词典可能会变得很大，而且值的分布可能会发生局部变化。针对这种情况，为了更轻松地容纳这部分更新，有时会为每块数据单独创建字典[[76](#76), [11](#11)]。字典压缩通常会将字符串谓词重写为整数谓词来优化查询，但这对于全局字典来说最容易实现。

​		字典压缩的一个好处是，如果系统将所有字典编码都制定为相同宽度的值，那么可以产生固定宽度的列。这需要牺牲部分存储上的收益，但这同时可以支持 CPU 的高效访问模式。实际上值得注意的是，即使朴素的 MonetDB 并未在其整个架构中利用压缩（在介绍 VectorWise 时有说明），但它仍对可变宽度的字符串列采用字典压缩，以便将其转换为固定宽度的列。

​		一个在工程实践中的考虑因素是如何有效地对字典进行压缩，而这往往取决于快速的散列方案。布谷鸟散列(cuckoo hashing)[[97](#97)]就是这样一种特别快速的技术方案。

#### 4.2.4 Frame Of Reference (FOR, 参考框)

​		如果列的分布具有值局部性，那么我们可以将其表示为：`基底值 + 增量值`。该基底可以作用于整个磁盘块，也可以作用于磁盘块中某个相对较小的段。而增量值是相对较小的整数（与较大的整数相比它只需要较少的位存储），因此 FOR 块的值的物理表示为基底值后续跟着许多较小的整数值。例如值序列：`1003,1001,1007,1006,1004` 可以表示为：`1000,3,1,7,6,4`。参考框也可以与增量编码(delta coding)相结合，其中当前值表示为相对于先前值的增量。当下一个值与上一个值高度相关时，这种编码相当有用，一个典型的例子就是由升序整数所组成的列表数据。

#### 4.2.5 The Patching Technique (修补技术)

​		当值域变得过大或者具有异常值时，字典编码和 FOR 的压缩率将会受到影响。但是如果值的分布频率是偏斜的(skewed)，那么可以仅对最频繁的值进行压缩，这种情况下我们仍可以压缩数据。

​		一个对 FOR 和字典编码的简单扩展是允许不压缩所谓的异常值。异常技术通常是通过将磁盘块分成两个相临近的部分来实现的：压缩编码从块的开始位置向前增长，而异常值数组从块末端向后增长。对于编码为异常值的元组，压缩编码使用特殊的转义值。但是如果我们使用 if-then-else 来进行检查的话，将很难预测算法内核中的分支，而这会导致在现代 CPU 上运行不佳（分支预测错误）。

​		修补技术[[100](#100)]并不在编码中存储转义值，而是使用这些值来维护链表。解压缩首先解压所有编码，而不管异常值。然后在第二步中则遍历链表，将异常值“修补”到解压缩的输出中。虽然这比单纯的异常测试需要做更多的工作，但是这将有问题的分支和主流程分离了开来，因此修补式的解压缩的性能更好。修补技术也可以视为一种为块处理提供优化机会的算法示例。

### 4.3 直接操作压缩数据

​		在很多情况下，除了一些面向行的算法之外，上述的面向列的压缩算法都可以直接进行操作而无需解压缩。由于系统可以通过读取更少的数据来节省 I/O 操作，而无需支付解压缩的成本，因此最终可以提高性能。对于游程编码之类的压缩方案，这种优点特别明显，这种压缩方案在单个压缩编码的列中组合了多个值。例如，如果游程编码的列中记录值“42”出现了1000次，那么当我们需要计算 SUM 聚合运算时，运算符可以简单地将值和游程的乘积作为 SUM 聚合的结果，而在此期间无需进行解压缩。另外一个例子是字典压缩所使用的保留顺序的编码，即保证表示列值的编码小于所有表示较大值的编码，并且大于所有表示较小值的编码。在这种情况下，在过滤期间的比较操作中（例如选择运算符）可以直接在编码上执行而无需解压缩数据，期间我们只需要对过滤器的相应边界/主元进行编码即可。

​		但是直接对压缩数据进行操作需要对查询执行引擎进行修改，查询运算符必须知道数据是如何压缩的，并相应地调整其处理数据的方式。这可能会导致高度不可扩展的代码，典型的情况是运算符可能由所有可能压缩方式的一组 if 语句所构成。解决该问题的一种方法是抽象化压缩算法的共有属性，以便于直接对其进行操作，从而使得运算符只需要考虑这些属性即可。这同时允许将新的压缩算法添加到系统中，而在此期间无需调整查询执行引擎代码。

​		这是通过在查询执行器中添加一个组件来完成的，该组件封装了压缩数据的中间表示形式，它被称为压缩块。压缩块中包含压缩格式的列数据缓冲区，并提供相应的 API，以允许查询运算符同构几种方式来访问该缓冲区。压缩块不一定需要拥有到存储块的相关映射，实际上压缩块的占用空间可能很小（例如单个 RLE 三元组），而且一个存储块通常可以分解成为多个压缩块。这些压缩块将键的属性暴露给查询运算符，例如 RLE 和位向量的压缩块倾向于描述单个列值的位置列表。类似于 Count 聚合运算符的查询运算符仅需要从压缩块的 API 中调用 **getSize()** 的方法即可，而无需遍历该块。和许多查询运算符高度相关的属性是 **isSorted()**、**isPositionContiguous()** 和 **isOneValue()**。根据这些属性，查询运算符可以选择性地提取有关该压缩块的顶层信息（例如 **getSize()**、**getFirstValue()** 和 **getEndPosition()** ），而不是通过遍历压缩块来提取一个值。

​		通过抽象化压缩方案的关键属性，可以直接对压缩数据进行操作，因此当向数据库系统添加其他压缩方案时，我们无需更改查询运算符。 如果工程师希望添加新的压缩方案，那么该工程师必须实现一个包含以下代码的接口：（a）将原始数据转换为压缩格式的代码；（b）从存储区域中扫描压缩数据并将其分解为压缩块的代码；（c）遍历压缩块并可选地获取解压缩数据值的代码；（d）根据在压缩块所使用压缩算法来提供的所有相关属性的值，以及（e）从压缩块中派生的顶层信息的代码（例如 **getSize()** ）。

​		文献中的实验结果表明，压缩不仅可以节省空间，而且可以显着提高性能。但是如果不对压缩数据直接进行操作的话，将很少会获得超过三倍的性能提升[[2](#2)]。而一旦使用了可扩展的压缩感知技术来扩展查询执行引擎，那么就可能获得超过一个数量级的性能提升，尤其是在已排序或具有某种顺序的列上。

### 4.4 延迟物化

​		在列存储中，有关逻辑实体（例如一个人）的信息存储在磁盘上的多个位置中（例如名称、电子邮件地址、电话号码等都存储在单独的列中），而在行存储中此类信息通常位于表的单行中。 但是大多数查询都从特定实体中访问多个属性。 此外，大多数数据库标准（例如 ODBC 和 JDBC ）都是输出每次访问一个实体的结果（而不是每次访问一列的结果）。 因此在大多数查询计划中，某些时候必须将来自多个列的数据组合在一起，以成为与实体相关的“行”。 因此元组的这种类似于连接的物化操作（也称为“元组构造”）在列存储中是非常普遍的。

​		原始的列存储[[38](#38), [40](#40)]逐列地将数据存储在磁盘或内存中，它们仅读取与查询相关的列（从磁盘或内存读入到 CPU 中），并使用这些相关的属性来构造元组， 然后在这些“行”上执行普通的行存储运算符以处理数据（例如选择、聚合和连接运算符）。 尽管在数据仓库所发现的分析型工作负载上，行存储仍可能胜过列存储，但这种在查询计划中过早构造元组的方法（“早期物化”）使得列存储数据库的许多性能潜力无法体现出来。

​		许多较新的列存储系统（例如 VectorWise、C-Store、Vertica 以及更小程度上的 SybaseIQ）选择将数据保留在列中，并且尽可能延后放入查询计划中，且直接对这些列进行操作。为此，通常需要构造中间的“位置”列表来匹配在不同列上执行的操作。以一个查询为例，该查询将谓词作用于两列，然后在作用谓词之后在同一表中投影到第三列。在使用延迟物化的列存储中，它们将谓词分别作用于每个属性的列，并生成满足谓词条件的值的位置列表（列内的垂直偏移量）。根据谓词的选择性要求，这个位置列表可以表示为简单的数组、位字符串（第 i 位中的 1表示第 i 个值已通过谓词要求）或位置范围的集合。然后根据这些位置获取交集（如果是位字符串则可以使用与操作）来创建单个位置列表。然后将该列表发送至第三列以提取所需位置的值。

​		**例子.** 图 4.1 展示了在现代列存储中延迟物化的查询计划以及执行的简单示例。 我们假设中间结果由位置列表表示，并且为了仅关注于延迟物化的问题且为了便于演示，我们暂时不使用压缩且以批量处理为例。图 4.1 中的查询是一个 select-project-join 查询，它实质上过滤了两个单独的表（R 和 S）中的三列，然后根据两列来连接这两个表，随后它对其中一个表 R 执行了 sum 聚合操作。 图 4.1 以图形的方式展示了回应该查询所执行的各个步骤，并显示了 MAL 代数中的查询计划以及每个 MAL 运算符的相应动作。

​		延迟物化意味着我们总是对单个列进行操作，即：在图 4.1 中，选择运算符独立地过滤每一列，从而最大程度地利用了内存带宽，因为每个操作符仅读取相关数据。在这种方式下，在图 4.1 的步骤 1 中将列 $R.a$ 过滤后，位置列表将包含所有满足要求的元组位置。回想一下，其位置以行 ID 的形式所记录。在图 4.1 中所有位置列表的中间结果均以虚线标记。然后在步骤 2 中，我们重建下一个过滤操作所需的列 $R.b$ 。由于位置列表 $ inter1$中的位置是有序的（正如我们在步骤 1 中顺序扫描 $R.a$ 一样），我们可以以一种缓存友好型的跳过式的顺序访问模式来投影满足要求的 $R.b$ 值。在步骤 3 中，我们扫描中间结果 $inter2$ 来作用于第二个过滤谓词（在 $R.b$ 上）。与之前的方式一样，这会产生一个新的中间数组，其中包含满足条件的相关位置，接着我们在步骤 4 中使用该位置从 $R.c$ 列中获取满足条件的值（这些值需要用于连接操作）。

​		随后，我们用与表 R 相同的方式来过滤 $S.a$ 列，并从另一个连接的输入列 $S.b$ 中获取满足条件的值。 然后在步骤 7 中，我们反转这些中间结果，以便以正确的顺序将其反馈到连接运算符中。步骤 8 中的连接运算符对每个连接的输入列进行了操作，并产生了两个位置列表，这些位置列表可用于投影到任何表在 select 子句上所需的任何属性。在这种情况下，由于我们只需要表 R 的位置列表，所以我们“忽略”了连接所产生结果中的右列，然后使用该位置列表来从 $R.a$ 列中获取聚合操作所需的满足要求的值。最后在步骤 11 中，我们执行聚合操作（再次一次处理一列），这可以享受 CPU 和高速缓存友好型的访问模式，因为我们仅读取相关数据。

​		在每次获取给定位置列表的列的值时（这是前一个运算符的结果），我们定义为执行了元组重构操作。 这种动作必须在查询计划内多次执行，即每个表至少执行 N - 1次，其中 N 是查询中所引用的表的属性数。元组在列之间对齐的和顺序的访问模式降低了元组重构操作的成本。 在图 4.1 中，我们展示了一种架构，该架构的中间结果以行 ID 的位置列表的形式出现。 但是正如我们前面所讨论的那样，还可以使用其它替代方案（通常取决于选择性），例如如文献[[80](#80)]中所述，使用位向量或单独过滤列并合并结果。

​		另一个有趣的观察结果是：通过 C-Store 的投影，元组重建变得更加轻量级了。假定每个投影根据单个主导列进行排序，那么在该列上的选择范围查询将立即可以把元组重建的操作限制为首个选择符定义的范围中。由于该投影根据该属性来进行排序，因此这是投影上的连续范围，这又意味着任何元组的重建操作仅在受限的水平分区中进行，而不是在整个投影中进行； 这自然可以提供更好的访问模式，因为这降低了缓存未命中率。 横向数据库分裂[[50](#50)]（将在后面讨论）提供了相同的效果，但是它使用了一种自组织的方式，即随着工作负载的演变从而对列进行部分排序、自适应工作负载模式并且避免提前创建整个投影。

​		**延迟物化的优点.** 延迟物化的优势主要在四个方面[[3](#3)]。 首先，选择和聚合运算符的倾向性使得一些元组的构建变得没必要。 因此，如果执行程序在构建元组之前等待了足够长的时间，那么它可能可以避免完全构建该元组的开销。其次，如果数据是使用面向列的压缩方法进行压缩的（可能允许压缩符号在一列中跨越多个值，例如 RLE），那么它必须在元组重建期间对数据进行解压缩，以便允许该列中的值可以与重建元组的其他列的值进行合并。 如上所述，这丧失了直接对压缩数据进行处理的优点。

​		第三，当对列数据直接进行处理时，由于给定的高速缓存相关行不会被周围的无关属性所污染，所以可以提高缓存性能[[5](#5)]。 随着内存和 CPU 之间的带宽日益成为现代计算系统的瓶颈，这一点显得尤其重要。例如，在where 子句中使用谓词评估操作时（例如 WHERE salary > $100,000），内存带宽不会浪费在集中传输到 CPU 的相同元组的其他属性上，因为只有 salary 属性才与该特定运算符相关。

​		第四，上述矢量化优化对固定长度属性的性能有较大影响。在行存储中，当元组中的任何属性为可变宽度时，那么整个元组将是可变宽度的。 在延迟物化的列存储中，固定宽度的列可以进行单独处理。

​		尽管有上述各个原因，但是延迟物化有时会比早期物化更慢（特别是如果使用原始的实现方法时）。 例如，如果对元组的许多属性使用非限制性谓词（例如，WHERE salary > $100 AND age > 5 AND ...），与简单地构造元组并避免延迟物化模型所造成的位置列表的计算相比，在表中计算大量交叠的位置中间数据（由多个谓词共同决定）、然后提取并物化大量满足所有谓词条件的元组的处理流程显然代价更高。

​		**多列块（Multi-column blocks）.** 在某些情况下，仍存在提升元组重建或完全消除该成本的几种优化方向。与一次存储单列相反，其主要思想是存储数据到多列构成的多组中，即所谓的多列块[[3](#3)]或矢量块(vector blocks)[[18](#18)]甚至列组(column-groups)[[11](#11)]。

​		一个多列块或矢量块包含一个来自特定关系的属性子集的可缓存水平分区，并使用其原始压缩形式进行存储。如图 4.2 所示。理解多列块的一种方法是想象其是一种类似于 PAX[[5](#5)]的存储格式，不同点在于关系表中的所有属性不需要在同一页面中。多列块允许断言分别应用于查询谓词所涉及的每个压缩列中，并将断言所应用的多个位置列表结果通过管道传递到一个相交运算符中（当它们仍在高速缓存中）并将结果输出到位置描述符中（如图 4.2 左侧所示），该描述符描述了哪些元组通过多所有断言（在示例中使用了位向量）。这些数据结构可以传递到更高级别的运算符中进一步处理。

​		虽然多列块没有消除相交位置的需要，但是每列只有少量子集同时被处理，这允许将断言的评估结果（位置列表）直接通过管道传递到相交运算符中，进而允许元组将仍存储在高速缓存中的多个值进行缝合重建。在不涉及任何连接操作的情况下，除了一些极端查询外[[3](#3)]，延迟物化都要比早期物化表现得更好。然而，在没有进一步优化的情况下，延迟物化的连接操作可能会出现问题，这部分内容将在下一部分进行讨论。

​		IBM Blink[[11](#11)] 使用的列组提出了一个更加灵活的布局，其中存储在同一页面的多列块的某些列也可以按照行存储格式（因此组成列组）。这里通过行格式存储并不意味着传统的在分页的行存储格式；取而代之的是，数据仍是以固定宽度的密集数组的形式存储，但是页面中列的子集可以“粘合”在一起，以形成矩阵。这可能会对需要同时处理这些列的运算符更有利，因为当查询无需获取其他属性时，这避免了存储中间结果以及元组重建的成本。

​		行式存储同样也出现了与列组和多组块类似的方向，例如多分辨率块[[94](#94)]。它的想法是，每个行存储页面可能是包含表的少量属性，这有助于在查询时避免在硬盘中加载不必要的属性数据。在其内部，页面仍以分页的形式进行组织，并在标准的行存储引擎中完成处理。

​		图 4.3 展示了现代列式存储数据库的各种可替代的存储格式。其中每页可以一次容纳单列，或者在内部以列式布局组织的多列，甚至可以以固定宽度的行粘合在一起。

​		大多数会现代列式存储数据库都支持列组及其变体，这包括 Vertica、VectorWise 以及 IBM BLU。多列块的一个弊端是需要事先做出组合决策，即决定哪些列在加载时组合在一起；这要求工作集的知识以及相当稳定的工作集负载模式。对 VectorWise 的研究工作表明，当访问模式方面的收益超过转换成本时，在查询过程中实时构建这些列组甚至更为有利[[101](#101)]，而预测系统则期望可以基于查询模式[[45](#45)]提供存储格式的可持续适应（即适当的列组）的相关特性。

### 4.5 连接

​		连接操作为列式存储提供了有效提升性能的机会，但是如果处理不当，这些机会也会导致瓶颈和复杂性。如果在连接操作中使用早期物化策略，那么在达到连接运算符之前就已经构造了元组，所以连接操作与行式存储数据库的处理操作相同（同时也产生与行式存储相同的连接操作性能）。然而，可以使用几种可供替代的算法与延迟物化策略一起使用。实现列式连接最直接的方法是将构成连接谓词的列输入到连接操作中。在典型的哈希连接中，这会产生更为紧凑的哈希表，进而在探查（probe）操作中获取更好的访问模式；较小的哈希表可以降低缓存未命中率。连接操作的输出是两个输入关系断言成功的一组位置对。例如，下图表示了大小分别为5和4的两列进行连接的结果：

​		对于许多连接算法，将对左（外）关系的输出位置进行排序，而对右（内）关系的输出位置则不进行排序。这是因为通常会顺序迭代左列的位置，而会根据连接谓词匹配探查右关系。对于其他连接算法（例如，对两组输入进行排序或者重新划分的算法）均不会对位置列表进行排序。而无论哪种方式，至少一组输出位置不会有序。无序的位置输出是有问题的，因为通常在连接后将需要使用该表中的其它列（例如，查询：

``` sql
SELECT emp.age, dept.name
FROM emp, dept
WHERE emp.dept_id = dept.id
```

需要在执行连接操作后，从 *emp* 表中取出 *age* 列以及从 *dept* 表中取出 *name* 列）。无序的位置查询是有问题的，因为这种无序方式从列中取值时需要在每个位置上跳转，而这会显著降低性能，因为大多数存储设备的随机访问要比顺序访问慢得多。

​		幸运的是，在一些研究文献中已经提出了一些改进措施，以避免在存储设备中反复跳转来提取值。一种方法是使用 “Jive 连接” [[64](#64), [89](#89)]。例如，当我们将上述大小分别为5和4的列连接在一起时，将会得到以下的位置输出：

​		右（内）表的位置列表是无序的。假设我们根据该位置列表中获取客户的 *name* 属性，其中包含以下四个客户：

​		Jive 连接的基本想法是在需要提取的位置列表中添加一列，即单调递增的整数序列：

​		然后输出即按照需要提取的位置列表进行排序（这种排序同时导致了新增列出现乱序）：

​		然后按顺序扫描表中的列，并提取（已排序）的位置列表的值添加到当前数据结构欧中。

​		最后将该数据结构再次排序，这次是在原始输出列中进行的，以将当前数据结构恢复为原始连接顺序（以便与其它表的连接输出相匹配）。

​		该算法可以在额外添加两次连接输出数据排序的成本下支持顺序迭代所有列。这部分额外的排序成本随着连接输出大小（即连接元组的数量）而增加。因为大多数数据库系统都有快速的外部排序算法实现（可以尽可能地顺序访问输入），所以相对于上述延迟物化连接的原始实现方法所导致的随机访问而言，该算法可以显著提升性能。

​		进一步的研究相比上述算法有额外的性能提升。其表明，在对连接输出的值进行提取时，无需对数据进行完全的排序来减少随机访问的性能花销。这是因为大多数存储介质都为连续的存储块，随机访问存储块要比跨块的随机访问便宜得多。因此，在使用数据库从列提取值时，数据库无需对位置列表进行完全排序，相反，只需要将其划分为存储块（或近似值）的位置即可。在每个分区内，位置可以保持无序，因为在存储块内的随机访问便宜得多（例如，内存和磁盘 I/O的差异，缓存和内存 I/O 的差异）。因此，我们在列取值时保持块的顺序访问，而不使用精确的顺序访问。Radix 连接[[17](#17)] 是这种优化方法的典型代表，其提供了一种在列取值前将列位置分区为块，并在列取值时重排序中间数据为原始顺序的快速机制，而该机制只要求数据都来自定宽列即可。

​		实际上，由于额外的工程复杂性，尽管在上述算法的文献中表现出了极好的性能，但是许多商业列式存储的实现并不实现纯粹的延迟物化连接算法。取而代之的是，对于按顺序访问左（外）输入并探查右（内）输入的连接算法，将使用混合的实现方法。对于右（内）关系，不仅仅输出构成连接谓词的列，所有相关的列（即，需要物化的列加上谓词描述的列）都在输入到连接运算符之前进行物化，而左（外）关系仅仅发送单个谓词描述的列。然后，连接的结果是来自右关系的一组元组和来自左关系的一组有序位置列表；来自左关系的位置用于检索该关系的其它列以完成元组构造过程。这种方法的优点是，仅仅物化来自左关系的对应连接谓词的元组值，同时避免了使用无序位置来物化右关系的值的代价。而对于不按顺序遍历两个输入关系的的连接算法而言，两个关系均在连接前进行物化。

​		上述的多列块提供了右（内）关系的候选表示方法。与其物化内部表的元组，不如将相关的列集合以一系列多列块的形式输入到连接运算符中。当内部表的值与匹配连接谓词时，该列的位置将用于检索同一块内其它列的值，并在运行时构造元组。当连接的选择性低并且仅需要构造少数元组时，该技术很有用，当条件不满足时则代价昂贵，因为它可能需要多次构造内部关系的特定元组。

​		最后，自2000年代初列式存储的重生以来，MonetDB 和 C-store 在连接算法上工作同时引发了针对内存型数据库进行高效连接的研究工作，例如[[9](#9), [10](#10), [6](#6)]。所有的这些研究的共同之处在于，它们遵循首先在列式存储操作中所采用的高级实践，例如专注于内存性能，对硬件特性和趋势敏感，对缓存敏感，利用 SIMD 指令，避免随机访问和指针追逐等。

### 4.6 分组，聚合和算术运算符

​		关于可能的关系运算符，除了元组重建外，到目前为止我们还讨论了选择运算符和连接运算符。在本节中，我们讨论列式存储的其余关系运算符，例如分组、聚合和算术运算符。总体而言，这些运算符利用延迟物化和矢量化以及基于固定宽度的密集数组的布局优势，因此可以利用 SIMD 指令、CPU友好和缓存友好的模式来处理相关数据。

​		**分组. ** 分组是现代列式存储中典型的基于哈希表的运算符，因此它可以利用上述章节讨论的类似属性。特别是我们可以创建一个更为紧凑的哈希表，即只能利用分组的相关属性，从而在探查操作时获得更好的访问模式。

​		**聚合.** 聚合操作大量使用了列式布局。特别是它们可以在相关列中使用紧密的 for 循环来工作。例如，在 **sum()**，**min()**，**max()**，**avg()** 运算符上，这些运算符只需扫描相关列（或同样以列式存储的中间数据），以最大限度地利用内存带宽。图 4.1 中的 Step 11 展示了示例，示例中我们可以看到 sum 运算符只能以列式布局访问相关数据。

​		**算术运算符.**  SQL 查询的 select 子句中可能使用其它运算符，即数学运算符（例如 **+,-,*,/** ）也利用列式布局来高效执行操作。然而，在这种情况下，由于此类运算符通常需要操作一组不同列，例如 **select A+B+C From R ...**，因此通常它们必须为每个操作物化中间结果。例如在我们之前的示例中，一个 **inter=add(A, B)** 运算符中会在列 A 和 B 上生成一个中间结果列 **inter**，然后该列会被传输至另一个 **res=add(C, inter)** 操作符中与列 C 执行相加操作并得到最终结果。矢量化有助于最小化中间结果的内存占比，但是同时也有研究表明，将中间结果动态转换为列组以便与多列(的矢量)一起工作也是有利的[[101](#101)]，这可以避免中间结果的物化。在上面的示例中，我们可以创建从 *(A, B, C)* 列中创建一个符合条件的列组，并一次性执行 sum 运算。

### 4.7 插入/更新/删除

​		从本质上讲，列存储相比行存储对更新操作更为敏感。将每列分别存储在单独的文件中意味着关系表中的每个元组都存储在多个文件中，即与表中的属性个数相同。在这种情况下，为了对元组执行更新操作需要多次 I/O 操作（和表中的属性个数相同）。相反，行式存储可以在一次 I/O 操作中执行一次更新操作。使用列组可以减少更新的成本，但是我们仍需要访问多个文件。

​		此外，除了使用垂直分片外，列式存储还大量使用了压缩技术，并且还可能以不同的值排序方式来存储多个表的副本或映射，所有的这些操作都可以提高分析查询的性能。即使用户想同时插入多个元组，由于有序或聚集的表存储，这部分磁盘 I/O 操作都是分散（随机）I/O 操作。最后，由于在数据写入磁盘前需要对数据进行解压缩、更新和重新压缩，因此压缩操作使得更新操作在计算上变得更加昂贵和复杂。如果更新的数据不再是原始位置，那么会产生额外的复杂性。

​		一些分析型的列式数据库系统，例如 C-Store 和 MonetDB，通过拆分架构为一个“读仓库”来管理所有分块数据和一个“写仓库”来管理最近的更新。因此所有查询都可以访问读仓库中的基础表信息以及写仓库的差分信息，并即时合并这些信息（MergeUnion 用于插入，MergeDiff 用于删除）。为了使得写仓库更小（通常驻留在 RAM 中），它的更新会周期性传播至读仓库中。

​		实现写仓库最自然的方法就是存储差异（插入、删除和更新）在内存结构中。MonetDB 使用普通列，即模式中的每个基本列都有两个辅助列来存储等待中的插入和删除，而更新则表述为删除后插入。C-Store 提出写优化仓库还可以使用行格式，这样可以更快地执行更新操作，因为每次只需要一次 I/O 操作就可以写入一个新行（但是以列格式合并更新会更加昂贵）。将增量存储在单独的表中的缺点在于，每个查询都必须在读仓库和差分表之间执行完全合并。但是还可以应用多种优化。例如，通常可以分别在读仓库和增量数据中执行查询（部分查询），并且仅在最后合并结果。例如，还可以将选择运算符独立应用于三列中（包括基础列，插入列，删除列），并且仅将符合要求的元组合并接着在查询计划中进一步推送。同样，删除可以通过使用布尔列来处理，布尔列通过使用压缩位图索引的某些可更新的变体来标记存储在RAM中给定元组的 active 状态。

​		VectorWise 系统使用一种称为位置增量树（Positional Delta Trees，PDT）的新型数据结构来存储数据差异。其关键优势在于，合并是基于对不同位置的知识，而不是基于表的排序键，因为排序键可能是复合且复杂的。当查询提交时，它将立即找出受影响的表位置。这样可以将合并活动从查询过程移动到更新过程中，这符合优化读处理的系统目标。相反，如果没有 PDT，我们将使用 CPU 昂贵的 MergeUnion/MergeDiff 来处理，而所有查询都需要重复执行来进行处理。除此之外，它还使得每个查询都读取有序的键列，而如果不需要这些属性来回应查询则会导致额外的 I/O。

​		跟踪有序表中的位置十分困难，因为插入/删除会在中途更改后续元组的位置。PDT 是一种计数型的 B树，可以在对数时间成本下追踪位置。

​		诸如 PDT 之类的差分数据结构，以及诸如先前提到的差分文件的方法，都是可以分层的：人们可以在增量的增量上再创建增量，等等。这种分层结构也可以在计算机的分层存储体系结构中得到应用，例如，在CPU缓存中放置很小的增量，在RAM中放置较大的增量，而在磁盘或固态存储器上放置极大的增量。除此之外，分层增量也是实现隔离和事务管理的工具。这种想法是，新事务在顶层中初始化创建空的 PDT 添加到已存在的 PDT 层中。而通过共享不变的较低层级的更大的 PDT，这可以提供廉价的快照隔离。随着事务的更改，这些更改将添加到顶层的 PDT 中，进而高效地捕获事务的写集。文献[[41](#41)]表述了在并发事务下保持 PDT 位置追踪一致性的算法，这种算法刚好适用于实现乐观并发控制（OCC）算法。

​		最后，最近的研究趋势是通过采用列式存储中率先提出的许多原理来进行快速 OLAP 处理，从而在单个系统中同时支持完整的 OLTP 和 OLAP 功能。System Hyper[[57](#57), [56](#56)] 是该领域最有代表性的示例，其主要设计特征是它依赖于硬件辅助的影子分页来避免在更新期间对页面进行锁定。除此之外，SAP HANA [[26](#26)]同时使用列格式和行格式来存储数据，以同时支持两种功能。

### 4.8 索引，自适应索引和数据库分裂

​		在本节中，我们讨论在列存储中的索引和自适应索引方法。即使列存储允许高效的扫描，其通常会远远胜于传统的行存储扫描，但通过适当地利用索引，仍然可以大大提升性能。在列存储中执行扫描可总结为简单地使用 for 循环遍历数组。尽管这可能非常有效并且对CPU友好，但是使用列存储索引可以快一个或多个数量级[[44](#44)]。 关于列存储索引的结构，已有文献表明：在全排序的列上进行工作要比在列顶部维护例如 AVL 树之类的内存树结构更为有益[[44](#44)]。树结构在遍历树时会带来随机访问，而另一方面，如果完全复制和排序基础列，我们可以在范围选择期间使用有效的二进制搜索算法。

​		**索引.** C-Store 提出了投影(projection)的概念，即多次复制每个表并且每个副本可以使用不同属性来进行排序。除此之外，每个副本不一定必须包含表的所有属性。查询可以使用单个覆盖投影(covering projection)，该覆盖投影在理想情况下由参与查询中的选择性谓词的属性进行排序，从而使搜索所需的信息最小化，并且使元组重建所需的信息最小化。由于列压缩效果非常好，与传统的行存储系统相比，对这些额外的投影进行物化不会带来显着的存储开销。 当然，所需的投影数量和种类取决于工作负载，拥有额外的投影会带来系统更新的开销（与传统系统中的索引情况非常类似）。

​		列存储中经常使用的另一种索引结构是区域图(zonemap)，即在每页基础上存储轻量级元数据，如最小和最大值。举个例子，Netezza 通过使用这种索引来加快扫描速度，即通过消除已知未满足要求的元组的相关页面。其他有趣的想法包括使用缓存敏感型(cache-conscious)位图索引[[86](#86)]，该索引为每个区域创建一个位图，而不是仅拥有最小和最大值信息。

​		**数据库分裂和自适应索引.** 所有类型的索引都通过使用空闲时间来进行设置工作并了解工作负载情况， 但是这会使得计算资源变得越来越窘迫。在本节的其余部分，我们将讨论有关列存储中数据库分裂[[44](#44)]的早期工作。数据库分裂在 MonetDB 系统的背景下率先在现代数据库系统中引入了自适应索引的概念，并专门为自适应索引量身定制了列存储架构[[48](#48), [49](#49), [50](#50), [51](#51), [37](#37), [35](#35), [36](#36), [83](#83)]。我们既讨论这些工作的基础，也讨论列存储架构的关键特性在列存储中繁荣发展的原因。

​		传统的非自适应索引方法的基本问题之一是：我们需要对创建的索引做出固定性的前期决策。但是由于时间和空间的限制，创建所有可能的索引是不可行的；因为没有足够的空间来存储所有可能的索引，但更重要的是，我们通常也没有足够的空闲时间来创建所有这些索引。在这种情况下，我们需要确定如何调整数据库系统的决策，即选择性地创建的所有可能索引的子集。但是在做出这样的决策前需要了解工作负载；我们需要知道我们将如何使用数据库系统，同时我们也要知道查询的类型，哪些数据对用户更加重要等。随着我们进入大数据时代，越来越多的应用场景表现出不可预测性行为（ad-hoc），这意味着没有工作负载知识可用于选择索引。除此之外，越来越多的应用要求尽快对新数据实现良好性能的目标。换句话说，我们没有时间花在分析预期的工作负载、调整系统和创建索引上。

​		这种动态的和在线的场景是自适应索引的主要动机。其主要思想是系统自主创建其所需索引。索引在创建时是（a）自适应的，即仅在需要时创建（b）部分的，即仅创建所需索引的片段，且（c）连续的，即系统持续适应。通过数据库分裂，我们可以在数据可用时立即使用数据库系统； 如果我们有足够的空闲时间和工作负载知识来充分准备当前工作负载所需的所有索引，那么当使用系统时间越长，就越能达到系统的最佳性能。

​		其主要的创新之处在于，物理数据存储随着每个传入的查询 *q* 的变化而不断变化，其通过使用 *q* 来作为应***如何存储物理数据的提示***。

​		假设查询请求 $A < 10$。作为响应，分裂的数据库系统在 $A$ 的所有元组中将 $A < 10$ 排在相应列 $C$ 的开头，同时将所有 A ≥ 10 的元组推到末尾。 随后的请求 $A ≥ v_1$ 的查询（其中 $v_1 ≥ 10$）必须搜索和分裂值 $A > 10$ 所在的列 $C$ 的最后一部分。同样，请求 $A < v_2$（其中 $v_2 ≤ 10$）的查询仅搜索和分裂列 $C$ 的第一部分。其中所有分裂动作都作为查询运算符的其中一部分来执行，而不需要外部管理。图 4.4 展示了这两个查询的示例，这些查询使用选择谓词来作为分区界限来分解一列。 查询 $Q1$ 将列切成三个分区，然后 $Q2$ 通过进一步切开第一块和最后一块来细化该分区（此时其上下限降低）。

​		分裂给列存储带来了性能的显著改善。例如，在最近进行的 Sloan Digital Sky Survey 实验中，它收集了来自天文学的相关查询和数据日志，其中带有分裂功能的 MonetDB 可以响应 160,000 个查询，而普通的 MonetDB 却仍在创建适当索引的途中，并且没有响应任何查询[[37](#37)]。类似的，在使用业务标准的 TPC-H 基准实验中，完美准备所有适当的索引/投影的 MonetDB 花费了3个小时，而使用了分裂功能的 MonetDB 则在零准备的情况下几秒钟内便响应了所有查询[[50](#50)]。 图 4.5 从文献[[50](#50)]中对 TPC-H 因子为 10 的实验分析中展示了这样一个例子。即使行存储使用了B树索引（MySQL presorted），普通的列存储系统（MonetDB）也要比行存储系统（MySQL）取得了更好的性能提升。使用了未压缩的列存储投影也可以进一步带来性能改进（MonetDB/presorted），但是系统初始化成本很高。在这些实验中，需要3个小时来准备 TPC-H 基准实验的完美投影。 另一方面，启用分裂功能后，MonetDB 可以立即开始处理查询，而无需进行任何准备。经过几次查询后，分裂达到了最佳性能，即类似于经过充分准备的系统（这些系统必须花费大量时间进行准备，此外还假定我们具有良好的工作负载预知识）。

​		术语“分裂”反映了这样一个事实：即数据库已被分区（分裂）为较小且易于管理的部分。分裂逐渐改善了数据访问，最终甚至在更新过程中[[49](#49)]也显着提高了查询处理的速度[[48](#48), [50](#50)]； 由于分裂是在列存储中所设计的，因此它是应用于属性级别上的；查询会导致重新组织其所引用的列，而不是完整的表；同时它也会根据需要通过多列进行传播，但这取决于查询对部分横向分解(*partial sideways cracking*)[[50](#50)]的需求，部分横向分解根据存储的限制来动态创建和删除多个分裂列的多个部分。在文献[[35](#35)]中，作者展示了如何通过有限的并发控制方法来支持并发查询，其完全依赖于闩锁(latch)，因为分裂的读取查询仅更改索引结构，而索引内容将保持不变。除此之外，随机分裂(stochastic cracking)[[37](#37)]通过不受限地遵循查询边界来执行不确定的分裂动作。在这种方式下，它可以使得分区在列的分布上更加均匀，从而阻止了较大的未索引分区的出现，而这些分区在未来分裂将十分昂贵。

​		随后的工作[[51](#51)]扩展了最初的分裂方法，其采用了类似于分区/合并的逻辑，并具有有效的排序步骤或较少的基数分区，同时它工作在列存储之上（每个属性都存储为矢量的集合，而不是单个数组）。虽然原始的分裂方法可以看作是增量式的快速排序（其中主元 pivot 由查询进行驱动），但这些最新的分裂变体探索了增量式快速排序和增量式外部合并排序之间的可能性，从而设计出一系列自适应索引算法（从非常活跃到非常懒惰）。

​		数据库分裂采用了完全不同的方法；直到到这种方式出现为止，查询处理时间此前被认为是难以打破的，即除了处理当前查询之外没有其他事情发生。而在另一方面，分裂持续进行并实时完善索引，从而获得实时和长期的性能优势。这是利用了某些列存储架构特性上的直接副作用。特别是批量处理和列式存储使得这些自适应索引思想变成了现实。通过一次将数据存储为一列，将其描述为密集的和固定宽度的数组，并存储在连续的内存区域中，这意味着与处理传统的分页（其定位单个元组甚至需要间接访问）相比，数据库分裂可以轻松地以最小的成本重排列数组。除此之外，批量处理意味着每个运算符在查询计划进入下一个运算符之前会完全消耗其输入列。对于数据库分裂而言，这意味着每个运算符都可以在单列上一次执行操作，这可以有效地执行所有细化操作。矢量化处理同样有效，其主要区别在于每个矢量都可以单独分裂，并且依赖于可以跨矢量移动的策略数据(policy data)[[51](#51)]。

​		在上部分章节讨论的 C-Store 的投影也是另一种形式上的索引，因为它们允许使用不同排序方法和不同属性来为不同的投影进行排序。从较高的层次上来说，分裂式架构，尤其是横向分裂式架构，可以看作是获得与 C-Store 投影相同结果的另一种方式，但是不同点在于它们是通过自适应方法来进行的，即我们不需要预先决定将要创建哪些投影，我们也不需要花费空闲时间来一次性创建所有投影。

​		除了不需要工作负载知识以及空闲时间所带来的好处之外，数据库分裂还使人们可以在使用数据库系统过程中无需进行复杂调整。 在这种情况下，它可以降低建立和使用数据库系统的成本，因为分裂数据库不需要数据库管理员根据工作负载的变化情况来制定索引的决策并维护索引的相关设置。

​		过去在数据库研究中还没有研究过像数据库分裂那样逐步适应索引结构的概念。 部分与分裂概念十分接近的一项工作是对部分索引的相关工作[[87](#87)]，它允许人们仅在表的一部分上创建传统的非自适应索引，从而避免可能对与查询不相关的数据建立索引（假设其具备良好的工作负载知识）。

### 4.9 总结和设计原理分类

​		本章所描述的设计原理已为大多数列存储数据库系统所采用，并为所有后续的内存型和缓存敏感型设计提供了基础背景知识。

​		*从其中的众多特性上可以明显看出，现代的列式存储超越了简单地一次存储一列的数据存储方法。它们提供了专为现代硬件和数据分析量身定制的全新数据库体系结构和执行引擎。*

​		在很多情况下，纯列式存储有助于最大程度地利用这些新的设计原理，即：当一次使用一列或使用矢量化时压缩将更加有效，而且当使用一次处理一列的方法时（即仅处理相关列且使用矢量化的运算符时），块处理将在最大程度上降低缓存未命中率和指令未命中率。从这个意义上讲，我们可以这样说：主要由 VectorWise 和 C-Store 所重新定义的现代列存储系统是一种包含所有这部分设计原理的系统，而不仅仅是面向列的存储布局。

​		正如我们在本章和前几章中讨论的内容所述，过去在传统行存储的背景下，对现代列式存储中的一些设计原理上进行了某种形式的研究。 但是在提出 MonetDB、VectorWise 和 C-Store 之前，没有系统能够提供这些在设计原理上的完整而全面的数据库系统设计和实现方法。从本质上讲，它们受到数据库社区中数据库系统架构数十年的研究启发，同时这也标志着需要彻底重新设计数据库内核。

​		图 4.6 提供了本章所讨论的一系列特性和设计原理总结，这些特性和设计原理共同定义了现代的列式存储，它同时指出了过去在行式存储中出现的相似但独立的特性。

|                               | 行式存储                                                     | 列式存储             |
| :---------------------------- | :----------------------------------------------------------- | :------------------- |
| **最小化位读取**              |                                                              |                      |
| （1）跳过未选择属性的读取     | 垂直分区，如[[12](#12)]<br />PAX[[5](#5)]<br />多分辨率块[[94](#94)]<br />列索引[[64](#64)] | 列式存储             |
| （2）仅处理运算符选定的属性   | 提供已规划索引，如[[69](#69), [25](#25)]<br />与(And)操作索引，如[[80](#80)] | 延迟物化             |
| （3）跳过未满足条件的值       | 索引，如[[34](#34)]<br />多维度聚类，如[[75](#75)]<br />区域图 | 投影分裂             |
| （4）跳过冗余位               | 压缩，如[[30](#30)]                                          | 列压缩               |
| （5）自适应索引/部分索引      | 部分索引[[87](#87)]                                          | 数据库分裂           |
| **最小化 CPU 时间**           |                                                              |                      |
| （1）最小化指令和数据未命中率 | 块处理[[74](#74)]<br />缓冲型运算符[[95](#95)]<br />缓存敏感型运算符[[85](#85)] | 矢量化执行           |
| （2）最小化每位读取的处理时间 | 直接操作压缩数据，如[[30](#30)]                              | 直接操作压缩列的数据 |
| （3）自定制运算符             | 编译查询计划，如[[70](#70)]                                  | RISC 风格型代数运算  |



## 5. 讨论、结论与未来方向

​		在本章中，我们将简要对比 MonetDB、VectorWise 和 C-Store。 我们还将讨论在面向行的数据库中模拟列存储的可行性，并提出结论和未来的工作。

### 5.1 对比 MonetDB/VectorWise/C-Store

​		为读优化而设计的数据库系统显然受益于 CPU 高效的查询执行方法。 为此，所有的三种架构（MonetDB，VectorWise 和 C-Store）都使用了某种形式上的面向块的处理方法[[74](#74)]，但它们使用的方式各不相同。MonetDB 通过一次处理一列的极端方式来处理查询，这意味着完全物化。而 C-Store 和 VectorWise 都允许流水线处理，它们使用元组块而不是在运算符之间传递单个元组。 在 C-Store 中，这主要是在压缩处理的过程中发生的，其中元组块将尽可能保持压缩格式。 相反，VectorWise 在整个处理过程和存储架构中遵循矢量化的处理策略。

​		在处理加载和更新数据时，MonetDB 和 C-Store 使用类似的方法，它们使用删除位图和包含插入（WOS）的临时表来处理。 特别是在连接的大型查询中，这会导致巨大的合并开销，因为在这种情况下，在执行连接运算前需要将插入和删除的操作应用于元组的处理流中。 VectorWise 的位置增量树（PDT）的数据结构可大大减少这种开销，尽管 PDT 的缺点是它们非常复杂，并且在执行插入和删除操作时的成本可能更高。

​		原始的 MonetDB 不使用压缩（除了自动处理的字符串字典之外），也不以任何特定顺序存储表的数据。VectorWise 和 C-Store 都大量使用压缩，但只有 C-Store 在处理过程中持续使用压缩数据。 C-Store 同时使用以不同排序顺序将数据多次存储在不同投影中的方法。 而 VectorWise 在所有列上使用了高度稀疏的索引，它在列数据上存储了元组范围的最小值和最大值，这可以减少范围谓词的 I/O， 它同时使用了协作扫描(cooperative scans)来进一步降低 I/O 压力。

### 5.2 模拟 列/行存储

​		关于面向列的数据库的一个常见问题是：是否有可能使用常规的面向行的系统来模拟基于列的系统。 有两种方法可以用来达成这种模拟：使用完全垂直分区并在每列上创建索引的方法[[1](#1)]。 我们将依次讨论每种可能的技术，并列出每种技术的相关缺点。

​		**垂直分区.** 在行存储中模拟列存储的方法中最直接方法是完全垂直划分每个表，和上文所述的早期列存储的方法相同[[58](#58)]。 在完全垂直分区的方法中，需要某种机制将同一行中的字段连接在一起（列存储通常使用相同的顺序来存储列进而隐式地关联元组，但是这种优化方法在行存储中是不可用的）。 因此最简单的方法是在每个表中添加一个整数类型的 Position 列（这通常比使用主键更可取，因为主键可能很大而且有可能是复合键）。例如，给定以下雇员表：

​		我们将添加一个简单的 Position 列：

​		然后，我们将为逻辑模式中的每一列创建一个物理表，其中物理表中都有两列，一列是逻辑模式中第 i 列的值，另一列是该值在原始表中的位置（该列通常是从1开始的密集整数序列）：

​		接着当从同一个关系中获取多个列时，需要重写查询并对 Position 进行连接处理。

​		这种方法的主要缺点是：（1）Position 数据的额外空间开销，以及由于额外的 I/O 操作所导致的在扫描这些列时的相关性能成本；（2）需要实现查询重写层来将原来的逻辑模式转换为到新的物理模式；（3）每次查询的连接操作数量激增，使得大多数行存储数据库系统所实现的优化器不堪重负，并导致其使用启发式方法，该方法偶尔会对查询性能造成灾难性的影响；（4）因为每一列都存储在单独的表中，所以每一列会为小得多的数据值存储一个可能大得多的元组 header（列存储则不为每一列都添加header，而是存储元组 header 到一个单独的列中）；（5）无法使用 RLE 之类的面向列的压缩方案（尽管该缺点可以通过一些巧妙的方法来缓解 -- [[20](#20)]）。

​		**在每一列上创建索引.** 这种方法涉及到为数据库系统中每个表的每一列单独创建一个索引。这解决了上面描述的大多数垂直分区的问题（至少包含问题（2，3，4）），但是它也有自己的问题。最明显的问题是大量的索引需要空间和更新的开销。但是，更微妙的问题是：每个索引通常不会按照它们在原始表中出现的顺序来存储这些值。因此将两个或多个属性物化为行时（如 4.4 节中所述，需要在查询期间的某个时刻处理）需要对 tuple-id 进行完整的连接（与垂直分区的情况不同，它可以通过简单地将两列合并在一起来执行该连接操作，而这两组值完全不需要对齐）。因此，在实际应用中，数据库优化器会使用原始的行存储表来进行列投影，而导致其根本无法利用列存储中只需读取必读列在 I/O 方面的好处。

### 5.3 结论

​		我们描述了许多在架构层面上的创新，这些创新使得现代的列存储（例如 MonetDB，VectorWise和C-Store）能够在分析型的工作负载上提供非常好的性能。这些创新包括压缩、矢量化、延迟物化以及高效的面向列的连接方法。这些想法已经进入多种商业产品和学术项目的派生项目（例如 VectorWise 和 Vertica）中以及其他的许多产品（例如 Aster Data、Greenplum、Infobright 和 Paraccel 等） 中。即使是在面向行的坚定支持者上，Oracle 也在其数据库应用 Exadata 中实现了一些面向列的技术（值得注意的是，他们已经实现了 PAX 页面布局和面向列的压缩）。与典型的数据仓库和分析型工作负载相比，这些产品在性能上比上一代行存储型系统高出一到两个数量级，并且在商业上取得了巨大成就（VectorWise，Vertica，Greenplum 和 Aster Data 最近都被收购了）。

​		尽管列式存储系统在学术和商业上取得巨大成就，但仍有一些有趣的方向可供将来研究，尤其是部分面向列的混合型系统有非常大的机会。 例如，将经常被同时访问的一组列存储在一起可能要比单纯使用列存储能提供更好的性能。除此之外，随着时间的流逝，根据访问模式在表的列和行上的布局之间进行自适应选择的系统可能会变得十分重要，因为要求用户确定其数据的布局类型实际上并不理想[[45](#45)]。 微软最近宣布了其 SQLServer 产品支持了矢量化查询处理的列式存储选项[[61](#61)]。 尽管这些功能仍然有限（系统是只读的，且仅对运算符和数据类型的子集进行矢量化处理，并且不做出任何自动数据布局决策），但他们朝着这个方向迈出的重要的一步。

​		我们同时还持续期待着面向列的想法可以在其他数据处理系统中能得到应用，例如越来越多地应用于大型数据集的分析式处理的 Hadoop/MapReduce[[27](#27)] 系统。

## 参考文献

1. <span name="1">Daniel J. Abadi, Samuel Madden, and Nabil Hachem. **Column-stores vs. row-stores: how different are they really?** In *Proceedings of the ACM SIGMOD Conference on Management of Data*, pages 967–980, 2008.</span>

2. <span name="2">Daniel J. Abadi, Samuel R. Madden, and Miguel Ferreira. **Integrating compression and execution in column-oriented database systems.** In *Proceedings of the ACM SIGMOD Conference on Management of Data*, pages 671–682, 2006. </span>

3. <span name="3">Daniel J. Abadi, Daniel S. Myers, David J. DeWitt, and Samuel R. Madden. **Materialization strategies in a column-oriented DBMS.** In *Proceedings of the International Conference on Data Endineering (ICDE)*, pages 466–475, 2007.</span>

4. <span name="4">R. Abdel Kader, P. Boncz, S. Manegold, and M. van Keulen. **ROX: run-time optimization of xqueries.** In *Proceedings of the ACM SIGMOD Conference on Management of Data*, pages 615–626. ACM, 2009. </span>

5. <span name="5">Anastassia Ailamaki, David J. DeWitt, Mark D. Hill, and Marios Skounakis. **Weaving relations for cache performance.** In *Proceedings of the International Conference on Very Large Data Bases (VLDB)*, pages 169–180, 2001.</span>

6. <span name="6">Martina-Cezara Albutiu, Alfons Kemper, and Thomas Neumann. **Massively Parallel Sort-Merge Joins in Main Memory Multi-Core Database Systems.** *Proceedings of the Very Large Data Bases Endowment (PVLDB)*, 5(10):1064–1075, 2012. </span>

7. <span name="7">Sihem Amer-Yahia and Theodore Johnson. **Optimizing queries on compressed bitmaps.** In *Proceedings of the International Conference on Very Large Data Bases (VLDB)*, pages 329–338, 2000. </span>

8. <span name="8">G. Antoshenkov. **Byte-aligned data compression.** U.S. Patent Number 5,363,098, 1994.</span>

9. <span name="9">Cagri Balkesen, Gustavo Alonso, Jens Teubner, and M. Tamer Özsu. **Multi-Core, Main-Memory Joins: Sort vs. Hash Revisited.** *Proceedings of the Very Large Data Bases Endowment (PVLDB)*, 7(1):85–96, 2013. </span>

10. <span name="10">Cagri Balkesen, Jens Teubner, Gustavo Alonso, and M. Tamer Özsu. **Main-memory hash joins on multi-core CPUs: Tuning to the underlying hardware.** In *Proceedings of the International Conference on Data Endineering (ICDE)*, pages 362–373, 2013. </span>

11. <span name="11">Ronald Barber, Peter Bendel, Marco Czech, Oliver Draese, Frederick Ho, Namik Hrle, Stratos Idreos, Min-Soo Kim, Oliver Koeth, Jae-Gil Lee, Tianchao Tim Li, Guy M. Lohman, Konstantinos Morfonios, René Müller, Keshava Murthy, Ippokratis Pandis, Lin Qiao, Vijayshankar Raman, Richard Sidle, Knut Stolze, and Sandor Szabo. **Business Analytics in (a) Blink.** *IEEE Data Eng. Bull.*, 35(1):9–14, 2012. </span>

12. <span name="12">Don S. Batory. **On searching transposed files.** *ACM Transactions on Database Systems*, 4(4):531–544, 1979. </span>

13. <span name="13">Philip A. Bernstein and Dah-Ming W. Chiu. **Using semi-joins to solve relational queries.** *J. ACM*, 28(1):25–40, 1981. </span>

14. <span name="14">R. Bestgen and T. McKinley. **Taming the business intelligence monster.** *IBM Systems Magazine*, 2007. </span>

15. <span name="15">Carsten Binnig, Stefan Hildenbrand, and Franz Färber. **Dictionary-based order-preserving string compression for main memory column stores.** In *Proceedings of the ACM SIGMOD Conference on Management of Data*, pages 283–296, 2009. </span>

16. <span name="16">Peter Boncz. **Monet: A next-generation DBMS kernel for query-intensive applications.** *University of Amsterdam, PhD Thesis*, 2002. </span>

17. <span name="17">Peter Boncz, Stefan Manegold, and Martin Kersten. **Database architecture optimized for the new bottleneck: Memory access.** In *Proceedings of the International Conference on Very Large Data Bases (VLDB)*, pages 54–65, 1999. </span>

18. <span name="18">Peter Boncz, Marcin Zukowski, and Niels Nes. **MonetDB/X100: Hyper-pipelining query execution.** In *Proceedings of the biennial Conference on Innovative Data Systems Research (CIDR)*, 2005. </span>

19. <span name="19">Peter A. Boncz and Martin L. Kersten. **MIL primitives for querying a fragmented world.** *VLDB Journal*, 8(2):101–119, 1999. </span>

20. <span name="20">Nicolas Bruno. **Teaching an old elephant new tricks.** In *Proceedings of the biennial Conference on Innovative Data Systems Research (CIDR)*, 2009. </span>

21. <span name="21">Surajit Chaudhuri and Vivek R. Narasayya. **An Effcient Cost-Driven Index Selection Tool for Microsoft SQL Server.** In *Proceedings of the International Conference on Very Large Data Bases (VLDB)*, pages 146–155, 1997. </span>

22. <span name="22">George P. Copeland and Setrag N. Khoshafian. **A decomposition storage model.** In *Proceedings of the ACM SIGMOD Conference on Management of Data*, pages 268–279, 1985. 

23. <span name="23">D. J. Dewitt, S. Ghandeharizadeh, D. A. Schneider, A. Bricker, H. I. Hsiao, and R. Rasmussen. **The gamma database machine project.** *IEEE Transactions on Knowledge and Data Engineering*, 2(1):44–62, 1990. </span>

24. <span name="24">David DeWitt. **From 1 to 1000 mips, November 2009.** PASS Summit 2009 Keynote. </span>

25. <span name="25">Amr El-Helw, Kenneth A. Ross, Bishwaranjan Bhattacharjee, Christian A. Lang, and George A. Mihaila. **Column-oriented query processing for row stores.** In *Proceedings of the International Workshop On Data Warehousing and OLAP*, pages 67–74, 2011. </span>

26. <span name="26">Franz Färber, Norman May, Wolfgang Lehner, Philipp Große, Ingo Müller, Hannes Rauhe, and Jonathan Dees. **The SAP HANA Database – An Architecture Overview.** *IEEE Data Eng. Bull.*, 35(1):28–33, 2012. </span>

27. <span name="27">Avrilia Floratou, Jignesh M. Patel, Eugene J. Shekita, and Sandeep Tata. **Column-Oriented Storage Techniques for MapReduce.** *Proceedings of the Very Large Data Bases Endowment (PVLDB)*, 4(7):419–429, 2011. </span>

28. <span name="28">Clark D. French. **“One Size Fits All” Database Architectures Do Not Work for DDS.** In *Proceedings of the ACM SIGMOD Conference on Management of Data*, pages 449–450, 1995. </span>

29. <span name="29">Clark D. French. **Teaching an OLTP Database Kernel Advanced Data Warehousing Techniques.** In *Proceedings of the International Conference on Data Endineering (ICDE)*, pages 194–198, 1997. </span>

30. <span name="30">G.Graefe and L.Shapiro. **Data compression and database performance.** In ACM/IEEE-CS Symp. On Applied Computing, pages 22 -27, April 1991. </span>

31. <span name="31">Jonathan Goldstein, Raghu Ramakrishnan, and Uri Shaft. **Compressing relations and indexes.** In *Proceedings of the International Conference on Data Endineering (ICDE)*, pages 370–379, 1998. </span>

32. <span name="32">Goetz Graefe. **Query evaluation techniques for large databases.** *ACM Computing Surveys*, 25(2):73–170, 1993. </span>

33. <span name="33">Goetz Graefe. **Effcient columnar storage in b-trees.** *SIGMOD Rec.*, 36(1):3–6, 2007. </span>

34. <span name="34">Goetz Graefe. **Modern B-Tree Techniques.** *Foundations and Trends in Databases*, 3(4):203–402, 2011. </span>

35. <span name="35">Goetz Graefe, Felix Halim, Stratos Idreos, Harumi Kuno, and Stefan Manegold. **Concurrency Control for Adaptive Indexing.** *Proceedings of the Very Large Data Bases Endowment (PVLDB)*, 5(7):656–667, 2012. </span>

36. <span name="36">Goetz Graefe, Stratos Idreos, Harumi Kuno, and Stefan Manegold. **Benchmarking Adaptive Indexing.** In *Proceedings of the TPC Technology Conference on Performance Evaluation and Benchmarking (TPCTC)*, pages 169–184, 2010. </span>

37. <span name="37">Felix Halim, Stratos Idreos, Panagiotis Karras, and Roland H. C. Yap. **Stochastic Database Cracking: Towards Robust Adaptive Indexing in Main-Memory Column-Stores.** *Proceedings of the Very Large Data Bases Endowment (PVLDB)*, 5(6):502–513, 2012. </span>

38. <span name="38">Alan Halverson, Jennifer L. Beckmann, Je rey F. Naughton, and David J. Dewitt. **A Comparison of C-Store and Row-Store in a Common Framework.** Technical Report TR1570, University of Wisconsin-Madison, 2006. </span>

39. <span name="39">Richard A. Hankins and Jignesh M. Patel. **Data morphing: an adaptive, cache-conscious storage technique.** In *Proceedings of the International Conference on Very Large Data Bases (VLDB)*, pages 417–428, 2003. </span>

40. <span name="40">Stavros Harizopoulos, Velen Liang, Daniel J. Abadi, and Samuel R. Madden. **Performance tradeoffs in read-optimized databases.** In *Proceedings of the International Conference on Very Large Data Bases (VLDB)*, pages 487–498, 2006. </span>

41. <span name="41">S. Héman, M. Zukowski, N.J. Nes, L. Sidirourgos, and P. Boncz. **Positional update handling in column stores.** In *Proceedings of the ACM SIGMOD Conference on Management of Data*, pages 543–554, 2010. </span>

42. <span name="42">William Hodak. **Exadata hybrid columnar compression.** Oracle Whitepaper, 2009. http://www.oracle.com/technetwork/database/exadata/index.html. </span>

43. <span name="43">Allison L. Holloway, Vijayshankar Raman, Garret Swart, and David J. DeWitt. **How to barter bits for chronons: compression and bandwidth trade offs for database scans.** In *Proceedings of the ACM SIGMOD Conference on Management of Data*, pages 389–400, 2007. </span>

44. <span name="44">Stratos Idreos. **Database Cracking: Towards Auto-tuning Database Kernels.** *CWI, PhD Thesis*, 2010. </span>

45. <span name="45">Stratos Idreos, Ioannis Alagiannis, Ryan Johnson, and Anastasia Ailamaki. **Here are my Data Files. Here are my Queries. Where are my Results?** In *Proceedings of the biennial Conference on Innovative Data Systems Research (CIDR)*, pages 57–68, 2011. </span>

46. <span name="45">Stratos Idreos, Fabian Groffen, Niels Nes, Stefan Manegold, Sjoerd Mullender, and Martin L Kersten. **MonetDB: Two Decades of Research in Column-oriented Database Architectures.** *IEEE Data Eng. Bull.*, 35(1):40–45, 2012. </span>

47. <span name="46">Stratos Idreos, Raghav Kaushik, Vivek R. Narasayya, and Ravishankar Ramamurthy. **Estimating the compression fraction of an index using sampling.** In *Proceedings of the International Conference on Data Endineering (ICDE)*, pages 441–444, 2010. </span>

48. <span name="48">Stratos Idreos, Martin L. Kersten, and Stefan Manegold. **Database cracking.** In *Proceedings of the biennial Conference on Innovative Data Systems Research (CIDR)*, pages 68–78, 2007. </span>

49. <span name="49">Stratos Idreos, Martin L. Kersten, and Stefan Manegold. **Updating a cracked database.** In *Proceedings of the ACM SIGMOD Conference on Management of Data*, pages 413–424, 2007. </span>

50. <span name="50">Stratos Idreos, Martin L. Kersten, and Stefan Manegold. **Self-organizing tuple reconstruction in column stores.** In *Proceedings of the ACM SIGMOD Conference on Management of Data*, pages 297–308, 2009. </span>

51. <span name="51">Stratos Idreos, Stefan Manegold, Harumi Kuno, and Goetz Graefe. **Merging What’s Cracked, Cracking What’s Merged: Adaptive Indexing in Main-Memory Column-Stores.** *Proceedings of the Very Large Data Bases Endowment (PVLDB)*, 4(9):585–597, 2011. </span>

52. <span name="52">Ryan Johnson, Vijayshankar Raman, Richard Sidle, and Garret Swart. **Row-wise parallel predicate evaluation.** *Proceedings of the Very Large Data Bases Endowment (PVLDB)*, 1(1):622–634, 2008. </span>

53. <span name="53">Theodore Johnson. **Performance measurements of compressed bitmap indices.** In *Proceedings of the International Conference on Very Large Data Bases (VLDB)*, pages 278–289, 1999. </span>

54. <span name="54">Ilkka Karasalo and Per Svensson. **The design of cantor: a new system for data analysis.** In *Proceedings of the 3rd international workshop on Statistical and scientific database management*, pages 224–244, 1986. </span>

55. <span name="55">Illka Karasalo and Per Svensson. **An overview of cantor: a new system for data analysis.** In *Proceedings of the 2nd international Workshop on Statistical Database Management (SSDBM)*, pages 315–324, 1983. </span>

56. <span name="56">Alfons Kemper and Thomas Neumann. **HyPer: A hybrid OLTP&OLAP main memory database system based on virtual memory snapshots.** In *Proceedings of the International Conference on Data Endineering (ICDE)*, pages 195–206, 2011. </span>

57. <span name="57">Alfons Kemper, Thomas Neumann, Florian Funke, Viktor Leis, and Henrik Mühe. **HyPer: Adapting Columnar Main-Memory Data Management for Transactional AND Query Processing.** *IEEE Data Eng. Bull.*, 35(1):46–51, 2012. </span>

58. <span name="58">Setrag Khoshafian, George Copeland, Thomas Jagodis, Haran Boral, and Patrick Valduriez. **A query processing strategy for the decomposed storage model.** In *Proceedings of the International Conference on Data Endineering (ICDE)*, pages 636–643, 1987. </span>

59. <span name="59">Setrag Khoshafian and Patrick Valduriez. **Parallel execution strategies for declustered databases.** In *Proceedings of the International Workshop on Database Machines*, pages 458–471, 1987. </span>

60. <span name="60">Andrew Lamb, Matt Fuller, Ramakrishna Varadarajan, Nga Tran, Ben Vandier, Lyric Doshi, and Chuck Bear. **The vertica analytic database: C-store 7 years later.** *Proceedings of the Very Large Data Bases Endow- ment (PVLDB)*, 5(12):1790–1801, 2012. </span>

61. <span name="61">P.Å. Larson, C.Clinciu, E.N. Hanson, A. Oks, S.L. Price, S. Rangarajan, A. Surna, and Q. Zhou. **Sql server column store indexes.** In *Proceedings of the ACM SIGMOD Conference on Management of Data*, pages 1177–1184, 2011. </span>

62. <span name="62">Per-Åke Larson, Cipri Clinciu, Campbell Fraser, Eric N. Hanson, Mostafa Mokhtar, Michal Nowakiewicz, Vassilis Papadimos, Susan L. Price, Srikumar Rangarajan, Remus Rusanu, and Mayukh Saubhasik. **Enhancements to sql server column stores.** In *Proceedings of the ACM SIGMOD Conference on Management of Data*, pages 1159–1168, 2013. </span>

63. <span name="63">Per-Åke Larson, Eric N. Hanson, and Susan L. Price. **Columnar Storage in SQL Server 2012.** *IEEE Data Eng. Bull.*, 35(1):15–20, 2012. </span>

64. <span name="64">Zhe Li and Kenneth A. Ross. **Fast joins using join indices.** *VLDB Journal*, 8:1–24, April 1999. </span>

65. <span name="65">R.A. Lorie and A.J. Symonds. **A relational access method for interactive applications.** In *Courant Computer Science Symposia, Vol. 6: Data Base Systems*. Prentice Hall, 1971. </span>

66. <span name="66">Roger MacNicol and Blaine French. **Sybase IQ multiplex - designed for analytics.** In *Proceedings of the International Conference on Very Large Data Bases (VLDB)*, pages 1227–1230, 2004. </span>

67. <span name="67">S. Manegold, P. Boncz, N. Nes, and M. Kersten. **Cache-conscious radix-decluster projections.** In *Proceedings of the International Conference on Very Large Data Bases (VLDB)*, pages 684–695, 2004. </span>

68. <span name="68">A. Mo at and J. Zobel. **Compression and fast indexing for multi-gigabyte text databases.** Australian Computer Journal*, 26(1):1–9, 1994. </span>

69. <span name="69">C. Mohan, Donald J. Haderle, Yun Wang, and Josephine M. Cheng. **Single Table Access Using Multiple Indexes: Optimization, Execution, and Concurrency Control Techniques.** pages 29–43, 1990. </span>

70. <span name="70">Thomas Neumann. **Effciently Compiling Effcient Query Plans for Modern Hardware.** *Proceedings of the Very Large Data Bases Endowment (PVLDB)*, 4(9):539–550, 2011. </span>

71. <span name="71">Patrick O’Neil and Dallan Quass. **Improved query performance with variant indexes.** In *Proceedings of the ACM SIGMOD Conference on Management of Data*, pages 38–49, 1997. </span>

72. <span name="72">Patrick E. O’Neil. **Model 204 architecture and performance.** In *Proceeding of the International Workshop on High Performance Transaction Systems*, pages 40–59, 1987. </span>

73. <span name="73">Patrick E. O’Neil, Elizabeth J. O’Neil, and Xuedong Chen. **The Star Schema Benchmark (SSB).** http://www.cs.umb.edu/~poneil/StarSchemaB.PDF. </span>

74. <span name="74">S. Padmanabhan, T. Malkemus, R. Agarwal, and A. Jhingran. **Block oriented processing of relational database operations in modern computer architectures.** In *Proceedings of the International Conference on Data Endineering (ICDE)*, pages 567–574, 2001. </span>

75. <span name="75">Sriram Padmanabhan, Bishwaranjan Bhattacharjee, Timothy Malkemus, Leslie Cranston, and Matthew Huras. **Multi-Dimensional Clustering: A New Data Layout Scheme in DB2.** In *Proceedings of the ACM SIGMOD Conference on Management of Data*, pages 637–641, 2003. </span>

76. <span name="76">M. Poess and D. Potapov. **Data compression in oracle.** In *Proceedings of the International Conference on Very Large Data Bases (VLDB)*, pages 937–947, 2003. </span>

77. <span name="77">Bogdan Raducanu, Peter A. Boncz, and Marcin Zukowski. **Micro adaptivity in vectorwise.** In *Proceedings of the ACM SIGMOD Conference on Management of Data*, pages 1231–1242, 2013. </span>

78. <span name="78">Ravishankar Ramamurthy, David Dewitt, and Qi Su. **A case for fractured mirrors.** In *Proceedings of the International Conference on Very Large Data Bases (VLDB)*, pages 89 – 101, 2002. </span>

79. <span name="79">V. Raman, G. Attaluri, R. Barber, N. Chainani, D. Kalmuk, V. KulandaiSamy, J. Leenstra, S. Lightstone, S. Liu, G. M. Lohman, T. Malkemus, R. Mueller, I. Pandis, B. Schiefer, D. Sharpe, R. Sidle, A. Storm, and L. Zhang. **DB2 with BLU Acceleration: So much more than just a column store.** *Proceedings of the Very Large Data Bases Endowment (PVLDB)*, 6(11), 2013. </span>

80. <span name="80">Vijayshankar Raman, Lin Qiao, Wei Han, Inderpal Narang, Ying-Lin Chen, Kou-Horng Yang, and Fen-Ling Ling. **Lazy, adaptive rid-list intersection, and its application to index anding.** In *Proceedings of the ACM SIGMOD Conference on Management of Data*, pages 773–784, 2007. </span>

81. <span name="81">Vijayshankar Raman, Garret Swart, Lin Qiao, Frederick Reiss, Vijay Dialani, Donald Kossmann, Inderpal Narang, and Richard Sidle. **Constant-Time Query Processing.** In *Proceedings of the International Conference on Data Endineering (ICDE)*, pages 60–69, 2008. </span>

82. <span name="82">Mark A. Roth and Scott J. Van Horn. **Database compression.** *SIGMOD Rec.*, 22(3):31–39, 1993. </span>

83. <span name="83">Felix Martin Schuhknecht, Alekh Jindal, and Jens Dittrich. **The Uncracked Pieces in Database Cracking.** *Proceedings of the Very Large Data Bases Endowment (PVLDB)*, 7(2), 2013. </span>

84. <span name="84">Minglong Shao, Jiri Schindler, Steven W. Schlosser, Anastassia Ailamaki, and Gregory R. Ganger. **Clotho: Decoupling memory page layout from storage organization.** In *Proceedings of the International Conference on Very Large Data Bases (VLDB)*, pages 696–707, 2004. </span>

85. <span name="85">Ambuj Shatdal, Chander Kant, and Je rey F. Naughton. **Cache Conscious Algorithms for Relational Query Processing.** In *Proceedings of the International Conference on Very Large Data Bases (VLDB)*, pages 510–521, 1994. </span>

86. <span name="86">Lefteris Sidirourgos and Martin L. Kersten. **Column imprints: a secondary index structure.** In *Proceedings of the ACM SIGMOD Conference on Management of Data*, pages 893–904, 2013. </span>

87. <span name="87">Michael Stonebraker. **The case for partial indexes.** *SIGMOD Record*, 18(4):4–11, 1989. </span>

88. <span name="88">Michael Stonebraker, Daniel J. Abadi, Adam Batkin, Xuedong Chen, Mitch Cherniack, Miguel Ferreira, Edmond Lau, Amerson Lin, Samuel R. Madden, Elizabeth J. O’Neil, Patrick E. O’Neil, Alexander Rasin, Nga Tran, and Stan B. Zdonik. **C-Store: A Column-Oriented DBMS.** In *Proceedings of the International Conference on Very Large Data Bases (VLDB)*, pages 553–564, 2005. </span>

89. <span name="89">Dimitris Tsirogiannis, Stavros Harizopoulos, Mehul A. Shah, Janet L. Wiener, and Goetz Graefe. **Query processing techniques for solid state drives.** In *Proceedings of the ACM SIGMOD Conference on Management of Data*, pages 59–72, 2009. </span>

90. <span name="90">Stephen Weyl, James Fries, Gio Wiederhold, and Frank Germano. **A modular self-describing clinical databank system.** *Computers and Biomedical Research*, 8(3):279 – 293, 1975. </span>

91. <span name="91">K. Wu, E. Otoo, and A. Shoshani. **Compressed bitmap indices for effcient query processing.** Technical Report LBNL-47807, 2001. </span>

92. <span name="92">K. Wu, E. Otoo, and A. Shoshani. **Compressing bitmap indexes for faster search operations.** In *Proceedings of the International Conference on Scientific and Statistical Database Management (SSDBM)*, pages 99–108, 2002. </span>

93. <span name="93">K. Wu, E. Otoo, A. Shoshani, and H. Nordberg. **Notes on design and implementation of compressed bit vectors.** Technical Report LBNL/PUB- 3161, 2001. </span>

94. <span name="94">Jingren Zhou and Kenneth A. Ross. **A Multi-Resolution Block Storage Model for Database Design.** In *Proceedings of the International Database Engineering and Applications Symposium (IDEAS)*, pages 22– 33, 2003. </span>

95. <span name="95">Jingren Zhou and Kenneth A. Ross. **Buffering Database Operations for Enhanced Instruction Cache Performance.** In *Proceedings of the ACM SIGMOD Conference on Management of Data*, pages 191–202, 2004. </span>

96. <span name="96">M. Zukowski. **Balancing vectorized query execution with bandwidth-optimized storage.** *University of Amsterdam, PhD Thesis*, 2009. </span>

97. <span name="97">M. Zukowski, S. Héman, and P. Boncz. **Architecture-conscious hashing.** In *Proceedings of the International Workshop on Data Management on New Hardware (DAMON)*, 2006. </span>

98. <span name="98">M. Zukowski, S. Héman, N. Nes, and P. Boncz. **Cooperative scans: dynamic bandwidth sharing in a DBMS.** In *Proceedings of the International Conference on Very Large Data Bases (VLDB)*, pages 723–734, 2007. </span>

99. <span name="99">Marcin Zukowski and Peter A. Boncz. **Vectorwise: Beyond column stores.** *IEEE Data Eng. Bull.*, 35(1):21–27, 2012. </span>

100. <span name="100">Marcin Zukowski, Sandor Heman, Niels Nes, and Peter Boncz. **Super-Scalar RAM-CPU Cache Compression.** In *Proceedings of the International Conference on Data Endineering (ICDE)*, 2006. </span>

101. <span name="101">Marcin Zukowski, Niels Nes, and Peter Boncz. **DSM vs. NSM: CPU performance tradeoffs in block-oriented query processing.** In *Proceedings of the International Workshop on Data Management on New Hardware (DAMON)*, pages 47–54, 2008. </span>

  
